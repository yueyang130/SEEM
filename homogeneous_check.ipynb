{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home2/22yuey/anaconda3/envs/DiT/lib/python3.10/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param Norm:  tensor(71695.8438, grad_fn=<NormBackward1>)\n",
      "tensor([2.8636e+12, 1.2621e+12, 1.4748e+12, 1.6022e+12, 4.3775e+11, 9.1026e+11,\n",
      "        2.7557e+12, 1.1846e+12, 1.9868e+12, 1.4116e+12],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([2.8636e+15, 1.2621e+15, 1.4747e+15, 1.6022e+15, 4.3773e+14, 9.1025e+14,\n",
      "        2.7557e+15, 1.1846e+15, 1.9868e+15, 1.4115e+15],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from TD3_BC import Critic\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "def restore_state_dict(model, param1):\n",
    "    # Pointer to keep track of where we are in the concatenated parameters\n",
    "    pointer = 0\n",
    "    state_dict = {}\n",
    "    assert param1.shape[0] == sum([torch.prod(torch.tensor(param.size())) for param in model.parameters()]), \"Size Mismatch\"\n",
    "    for name, param in model.named_parameters():\n",
    "        # Compute the number of elements in this parameter\n",
    "        num_elements = torch.prod(torch.tensor(param.size()))\n",
    "\n",
    "        # Slice the concatenated parameters to get the values for this parameter\n",
    "        param_values = param1[pointer:pointer + num_elements]\n",
    "\n",
    "        # Reshape the values to match the original shape\n",
    "        param_values = param_values.view(param.size())\n",
    "\n",
    "        # Add to the state_dict\n",
    "        state_dict[name] = param_values\n",
    "\n",
    "        # Move the pointer\n",
    "        pointer += num_elements\n",
    "\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "N = 100\n",
    "lambda_scale = 10\n",
    "\n",
    "env_name = 'hopper-medium-replay-v2'\n",
    "# env_name = 'antmaze-large-diverse-v0'\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "_max_action = float(env.action_space.high[0])\n",
    "\n",
    "model_path = f'results/{env_name}.pt'\n",
    "model_list = torch.load(model_path)['q1_models']\n",
    "critic_param = model_list[-1]\n",
    "model = Critic(state_dim, action_dim, layernorm=0).q1\n",
    "restore_state_dict(model, critic_param)\n",
    "\n",
    "# critic = torch.load(model_path)['grad_']\n",
    "    \n",
    "    # torch.save({\n",
    "    #     'steps': steps,\n",
    "    #     'q1_models': q1_models,\n",
    "    #     'ntks': ntk_list,\n",
    "    #     'random_batch_grad_list': random_batch_grad_list,\n",
    "    #     'fix_batch_grad_list': fix_batch_grad_list,\n",
    "    #     'pi_list': pi_list,\n",
    "    # }, f'results/{args.env}.pt')\n",
    "    \n",
    "\n",
    "# Randomly sample N=100 points from [-1,1]^2\n",
    "X = torch.tensor(np.random.uniform(-1, 1, (N, state_dim + action_dim)), dtype=torch.float32)\n",
    "# X_prime = torch.tensor(np.random.uniform(-1, 1, (N, state_dim + action_dim)), dtype=torch.float32)\n",
    "# add guassian noise to X and make the sum bounded in [-1,1]\n",
    "X_prime = X + torch.tensor(np.random.normal(0, 10, (N, state_dim + action_dim)), dtype=torch.float32)\n",
    "X_prime = torch.clamp(X_prime, -1, 1)\n",
    "# print(X_prime[:10])\n",
    "y = model(X)\n",
    "\n",
    "print('param Norm: ', torch.norm(critic_param))\n",
    "# print(X[:10])\n",
    "print(y.squeeze()[:10])\n",
    "\n",
    "# Multiply MLP parameters by lambda and generate y_scaled\n",
    "for param in model.parameters():\n",
    "    param.data *= lambda_scale\n",
    "\n",
    "y_scaled = model(X)\n",
    "print(y_scaled.squeeze()[:10])\n",
    "# print(y_scaled.squeeze() / y.squeeze())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value Linear regression coefficient: 999.99225\n",
      "Expected coefficient: 1000\n",
      "R^2 score: 0.9999999999242609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y and y_scaled for linear regression\n",
    "y_np = y.detach().numpy().reshape(-1, 1)\n",
    "y_scaled_np = y_scaled.detach().numpy().reshape(-1, 1)\n",
    "\n",
    "# Perform linear regression using least squares\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(y_np, y_scaled_np)\n",
    "\n",
    "# Predict y_scaled\n",
    "y_pred = reg.predict(y_np)\n",
    "\n",
    "# Compute R^2 score\n",
    "r2 = r2_score(y_scaled_np, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Q-value Linear regression coefficient:\", reg.coef_[0][0])\n",
    "print(f\"Expected coefficient: {lambda_scale**3}\")\n",
    "print(f\"R^2 score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 Weight Norm / Bias Norm: 10.316726473552626\n",
      "Weight Norm: 1116.1197509765625\n",
      "Bias Norm: 108.18545532226562\n",
      "iter1 Weight Norm / Bias Norm: 10.470930314846687\n",
      "Weight Norm: 1964.2047119140625\n",
      "Bias Norm: 187.58645629882812\n",
      "iter2 Weight Norm / Bias Norm: 10.534145979571308\n",
      "Weight Norm: 2753.79736328125\n",
      "Bias Norm: 261.4162902832031\n",
      "iter3 Weight Norm / Bias Norm: 10.567933979296413\n",
      "Weight Norm: 3519.4296875\n",
      "Bias Norm: 333.02911376953125\n",
      "iter4 Weight Norm / Bias Norm: 10.585212842874496\n",
      "Weight Norm: 4267.6962890625\n",
      "Bias Norm: 403.17529296875\n",
      "iter5 Weight Norm / Bias Norm: 10.595255085036957\n",
      "Weight Norm: 5006.193359375\n",
      "Bias Norm: 472.493896484375\n",
      "iter6 Weight Norm / Bias Norm: 10.598413576373021\n",
      "Weight Norm: 5734.1279296875\n",
      "Bias Norm: 541.0364379882812\n",
      "iter7 Weight Norm / Bias Norm: 10.597772981709555\n",
      "Weight Norm: 6454.48876953125\n",
      "Bias Norm: 609.0419921875\n",
      "iter8 Weight Norm / Bias Norm: 10.588504054374566\n",
      "Weight Norm: 7163.556640625\n",
      "Bias Norm: 676.5409545898438\n",
      "iter9 Weight Norm / Bias Norm: 10.577690481638538\n",
      "Weight Norm: 7866.33349609375\n",
      "Bias Norm: 743.672119140625\n",
      "iter10 Weight Norm / Bias Norm: 10.565564702733615\n",
      "Weight Norm: 8563.1748046875\n",
      "Bias Norm: 810.4796142578125\n",
      "iter11 Weight Norm / Bias Norm: 10.549464463406524\n",
      "Weight Norm: 9251.2177734375\n",
      "Bias Norm: 876.9371948242188\n",
      "iter12 Weight Norm / Bias Norm: 10.529929363928447\n",
      "Weight Norm: 9930.130859375\n",
      "Bias Norm: 943.0386962890625\n",
      "iter13 Weight Norm / Bias Norm: 10.50963988237946\n",
      "Weight Norm: 10603.142578125\n",
      "Bias Norm: 1008.8968505859375\n",
      "iter14 Weight Norm / Bias Norm: 10.484001919769627\n",
      "Weight Norm: 11263.462890625\n",
      "Bias Norm: 1074.34765625\n",
      "iter15 Weight Norm / Bias Norm: 10.455062777049328\n",
      "Weight Norm: 11914.658203125\n",
      "Bias Norm: 1139.6065673828125\n",
      "iter16 Weight Norm / Bias Norm: 10.422312562638572\n",
      "Weight Norm: 12553.49609375\n",
      "Bias Norm: 1204.4827880859375\n",
      "iter17 Weight Norm / Bias Norm: 10.384402228978647\n",
      "Weight Norm: 13176.625\n",
      "Bias Norm: 1268.88623046875\n",
      "iter18 Weight Norm / Bias Norm: 10.341444618395267\n",
      "Weight Norm: 13786.1240234375\n",
      "Bias Norm: 1333.0946044921875\n",
      "iter19 Weight Norm / Bias Norm: 10.295057921819744\n",
      "Weight Norm: 14380.4541015625\n",
      "Bias Norm: 1396.830810546875\n",
      "iter20 Weight Norm / Bias Norm: 10.244189700763727\n",
      "Weight Norm: 14957.638671875\n",
      "Bias Norm: 1460.1094970703125\n",
      "iter21 Weight Norm / Bias Norm: 10.19324518806994\n",
      "Weight Norm: 15528.025390625\n",
      "Bias Norm: 1523.3642578125\n",
      "iter22 Weight Norm / Bias Norm: 10.14604195838202\n",
      "Weight Norm: 16096.4404296875\n",
      "Bias Norm: 1586.474853515625\n",
      "iter23 Weight Norm / Bias Norm: 10.107301169775829\n",
      "Weight Norm: 16675.75390625\n",
      "Bias Norm: 1649.8720703125\n",
      "iter24 Weight Norm / Bias Norm: 10.075876096511768\n",
      "Weight Norm: 17268.544921875\n",
      "Bias Norm: 1713.8504638671875\n",
      "iter25 Weight Norm / Bias Norm: 10.059482265201792\n",
      "Weight Norm: 17890.640625\n",
      "Bias Norm: 1778.4852294921875\n",
      "iter26 Weight Norm / Bias Norm: 10.054050446836554\n",
      "Weight Norm: 18538.201171875\n",
      "Bias Norm: 1843.85400390625\n",
      "iter27 Weight Norm / Bias Norm: 10.056217181746439\n",
      "Weight Norm: 19204.17578125\n",
      "Bias Norm: 1909.681884765625\n",
      "iter28 Weight Norm / Bias Norm: 10.06694585588365\n",
      "Weight Norm: 19891.05859375\n",
      "Bias Norm: 1975.878173828125\n",
      "iter29 Weight Norm / Bias Norm: 10.08118156614035\n",
      "Weight Norm: 20588.0703125\n",
      "Bias Norm: 2042.2279052734375\n",
      "iter30 Weight Norm / Bias Norm: 10.098245850463995\n",
      "Weight Norm: 21294.30859375\n",
      "Bias Norm: 2108.713623046875\n",
      "iter31 Weight Norm / Bias Norm: 10.116767502723393\n",
      "Weight Norm: 22006.54296875\n",
      "Bias Norm: 2175.25439453125\n",
      "iter32 Weight Norm / Bias Norm: 10.135383800450164\n",
      "Weight Norm: 22721.541015625\n",
      "Bias Norm: 2241.8037109375\n",
      "iter33 Weight Norm / Bias Norm: 10.157051872965313\n",
      "Weight Norm: 23470.603515625\n",
      "Bias Norm: 2310.769287109375\n",
      "iter34 Weight Norm / Bias Norm: 10.178822819085523\n",
      "Weight Norm: 24201.26953125\n",
      "Bias Norm: 2377.60986328125\n",
      "iter35 Weight Norm / Bias Norm: 10.199215304577416\n",
      "Weight Norm: 24927.86328125\n",
      "Bias Norm: 2444.09619140625\n",
      "iter36 Weight Norm / Bias Norm: 10.218634727343291\n",
      "Weight Norm: 25655.107421875\n",
      "Bias Norm: 2510.619873046875\n",
      "iter37 Weight Norm / Bias Norm: 10.237000983063401\n",
      "Weight Norm: 26381.71875\n",
      "Bias Norm: 2577.094482421875\n",
      "iter38 Weight Norm / Bias Norm: 10.254359874214417\n",
      "Weight Norm: 27107.8359375\n",
      "Bias Norm: 2643.54248046875\n",
      "iter39 Weight Norm / Bias Norm: 10.271082990897126\n",
      "Weight Norm: 27834.537109375\n",
      "Bias Norm: 2709.990478515625\n",
      "iter40 Weight Norm / Bias Norm: 10.28696537026309\n",
      "Weight Norm: 28560.583984375\n",
      "Bias Norm: 2776.3857421875\n",
      "iter41 Weight Norm / Bias Norm: 10.302207650964014\n",
      "Weight Norm: 29287.42578125\n",
      "Bias Norm: 2842.830078125\n",
      "iter42 Weight Norm / Bias Norm: 10.316508588073013\n",
      "Weight Norm: 30012.876953125\n",
      "Bias Norm: 2909.208740234375\n",
      "iter43 Weight Norm / Bias Norm: 10.3303779112461\n",
      "Weight Norm: 30738.912109375\n",
      "Bias Norm: 2975.58447265625\n",
      "iter44 Weight Norm / Bias Norm: 10.34364025192728\n",
      "Weight Norm: 31464.732421875\n",
      "Bias Norm: 3041.93994140625\n",
      "iter45 Weight Norm / Bias Norm: 10.35627044398373\n",
      "Weight Norm: 32189.986328125\n",
      "Bias Norm: 3108.260498046875\n",
      "iter46 Weight Norm / Bias Norm: 10.368522166521636\n",
      "Weight Norm: 32915.640625\n",
      "Bias Norm: 3174.573974609375\n",
      "iter47 Weight Norm / Bias Norm: 10.380484183072237\n",
      "Weight Norm: 33641.80859375\n",
      "Bias Norm: 3240.870849609375\n",
      "iter48 Weight Norm / Bias Norm: 10.391645786003954\n",
      "Weight Norm: 34366.578125\n",
      "Bias Norm: 3307.13525390625\n",
      "iter49 Weight Norm / Bias Norm: 10.402601306023065\n",
      "Weight Norm: 35091.93359375\n",
      "Bias Norm: 3373.380615234375\n",
      "iter50 Weight Norm / Bias Norm: 10.412956008126748\n",
      "Weight Norm: 35816.8671875\n",
      "Bias Norm: 3439.64453125\n",
      "iter51 Weight Norm / Bias Norm: 10.423132728794604\n",
      "Weight Norm: 36542.69921875\n",
      "Bias Norm: 3505.9228515625\n",
      "iter52 Weight Norm / Bias Norm: 10.432974054799113\n",
      "Weight Norm: 37269.140625\n",
      "Bias Norm: 3572.2451171875\n",
      "iter53 Weight Norm / Bias Norm: 10.44258197173958\n",
      "Weight Norm: 37995.20703125\n",
      "Bias Norm: 3638.48779296875\n",
      "iter54 Weight Norm / Bias Norm: 10.451837267106777\n",
      "Weight Norm: 38721.44921875\n",
      "Bias Norm: 3704.75048828125\n",
      "iter55 Weight Norm / Bias Norm: 10.460812400128084\n",
      "Weight Norm: 39447.76953125\n",
      "Bias Norm: 3771.00439453125\n",
      "iter56 Weight Norm / Bias Norm: 10.469398328336004\n",
      "Weight Norm: 40173.7421875\n",
      "Bias Norm: 3837.254150390625\n",
      "iter57 Weight Norm / Bias Norm: 10.477770810340477\n",
      "Weight Norm: 40899.7890625\n",
      "Bias Norm: 3903.48193359375\n",
      "iter58 Weight Norm / Bias Norm: 10.485913315730475\n",
      "Weight Norm: 41625.84765625\n",
      "Bias Norm: 3969.692138671875\n",
      "iter59 Weight Norm / Bias Norm: 10.49390014253185\n",
      "Weight Norm: 42352.3203125\n",
      "Bias Norm: 4035.89892578125\n",
      "iter60 Weight Norm / Bias Norm: 10.501597410623566\n",
      "Weight Norm: 43078.578125\n",
      "Bias Norm: 4102.09765625\n",
      "iter61 Weight Norm / Bias Norm: 10.509058330928024\n",
      "Weight Norm: 43804.875\n",
      "Bias Norm: 4168.296875\n",
      "iter62 Weight Norm / Bias Norm: 10.51638771489864\n",
      "Weight Norm: 44531.5\n",
      "Bias Norm: 4234.486328125\n",
      "iter63 Weight Norm / Bias Norm: 10.523464281282312\n",
      "Weight Norm: 45257.828125\n",
      "Bias Norm: 4300.65869140625\n",
      "iter64 Weight Norm / Bias Norm: 10.530269978521272\n",
      "Weight Norm: 45983.89453125\n",
      "Bias Norm: 4366.82958984375\n",
      "iter65 Weight Norm / Bias Norm: 10.536898184144894\n",
      "Weight Norm: 46709.91015625\n",
      "Bias Norm: 4432.98486328125\n",
      "iter66 Weight Norm / Bias Norm: 10.54349816626202\n",
      "Weight Norm: 47436.67578125\n",
      "Bias Norm: 4499.14013671875\n",
      "iter67 Weight Norm / Bias Norm: 10.549844407188447\n",
      "Weight Norm: 48163.15625\n",
      "Bias Norm: 4565.29541015625\n",
      "iter68 Weight Norm / Bias Norm: 10.55600582209465\n",
      "Weight Norm: 48889.80078125\n",
      "Bias Norm: 4631.4677734375\n",
      "iter69 Weight Norm / Bias Norm: 10.561974473612189\n",
      "Weight Norm: 49616.06640625\n",
      "Bias Norm: 4697.61279296875\n",
      "iter70 Weight Norm / Bias Norm: 10.567864344904764\n",
      "Weight Norm: 50342.6328125\n",
      "Bias Norm: 4763.7470703125\n",
      "iter71 Weight Norm / Bias Norm: 10.573544643188415\n",
      "Weight Norm: 51068.51171875\n",
      "Bias Norm: 4829.83837890625\n",
      "iter72 Weight Norm / Bias Norm: 10.57911771951048\n",
      "Weight Norm: 51795.1640625\n",
      "Bias Norm: 4895.9814453125\n",
      "iter73 Weight Norm / Bias Norm: 10.58453734110881\n",
      "Weight Norm: 52521.5234375\n",
      "Bias Norm: 4962.09912109375\n",
      "iter74 Weight Norm / Bias Norm: 10.589784472131175\n",
      "Weight Norm: 53247.96484375\n",
      "Bias Norm: 5028.23876953125\n",
      "iter75 Weight Norm / Bias Norm: 10.594986483047618\n",
      "Weight Norm: 53974.39453125\n",
      "Bias Norm: 5094.33349609375\n",
      "iter76 Weight Norm / Bias Norm: 10.600088602035122\n",
      "Weight Norm: 54701.25\n",
      "Bias Norm: 5160.4521484375\n",
      "iter77 Weight Norm / Bias Norm: 10.605046755479687\n",
      "Weight Norm: 55427.8671875\n",
      "Bias Norm: 5226.5556640625\n",
      "iter78 Weight Norm / Bias Norm: 10.609774881975687\n",
      "Weight Norm: 56153.5390625\n",
      "Bias Norm: 5292.623046875\n",
      "iter79 Weight Norm / Bias Norm: 10.61449889531714\n",
      "Weight Norm: 56880.046875\n",
      "Bias Norm: 5358.71240234375\n",
      "iter80 Weight Norm / Bias Norm: 10.619094269544513\n",
      "Weight Norm: 57606.3671875\n",
      "Bias Norm: 5424.791015625\n",
      "iter81 Weight Norm / Bias Norm: 10.623575586234567\n",
      "Weight Norm: 58332.80859375\n",
      "Bias Norm: 5490.8828125\n",
      "iter82 Weight Norm / Bias Norm: 10.627938202766998\n",
      "Weight Norm: 59058.94921875\n",
      "Bias Norm: 5556.95263671875\n",
      "iter83 Weight Norm / Bias Norm: 10.63169509447043\n",
      "Weight Norm: 59783.8125\n",
      "Bias Norm: 5623.16845703125\n",
      "iter84 Weight Norm / Bias Norm: 10.634205634745596\n",
      "Weight Norm: 60506.44921875\n",
      "Bias Norm: 5689.794921875\n",
      "iter85 Weight Norm / Bias Norm: 10.636684052023066\n",
      "Weight Norm: 61229.65625\n",
      "Bias Norm: 5756.4609375\n",
      "iter86 Weight Norm / Bias Norm: 10.639101813408635\n",
      "Weight Norm: 61952.6015625\n",
      "Bias Norm: 5823.1044921875\n",
      "iter87 Weight Norm / Bias Norm: 10.641606027128246\n",
      "Weight Norm: 62676.6796875\n",
      "Bias Norm: 5889.7763671875\n",
      "iter88 Weight Norm / Bias Norm: 10.644234670949505\n",
      "Weight Norm: 63401.625\n",
      "Bias Norm: 5956.4287109375\n",
      "iter89 Weight Norm / Bias Norm: 10.646809402038107\n",
      "Weight Norm: 64127.10546875\n",
      "Bias Norm: 6023.12890625\n",
      "iter90 Weight Norm / Bias Norm: 10.649406567149818\n",
      "Weight Norm: 64852.83203125\n",
      "Bias Norm: 6089.80712890625\n",
      "iter91 Weight Norm / Bias Norm: 10.651828350837086\n",
      "Weight Norm: 65577.6015625\n",
      "Bias Norm: 6156.46435546875\n",
      "iter92 Weight Norm / Bias Norm: 10.654288932569512\n",
      "Weight Norm: 66303.3671875\n",
      "Bias Norm: 6223.162109375\n",
      "iter93 Weight Norm / Bias Norm: 10.656737408285323\n",
      "Weight Norm: 67028.828125\n",
      "Bias Norm: 6289.8076171875\n",
      "iter94 Weight Norm / Bias Norm: 10.65918171607241\n",
      "Weight Norm: 67755.140625\n",
      "Bias Norm: 6356.5048828125\n",
      "iter95 Weight Norm / Bias Norm: 10.661534692178455\n",
      "Weight Norm: 68480.84375\n",
      "Bias Norm: 6423.16943359375\n",
      "iter96 Weight Norm / Bias Norm: 10.663775135282846\n",
      "Weight Norm: 69206.375\n",
      "Bias Norm: 6489.85693359375\n",
      "iter97 Weight Norm / Bias Norm: 10.665953319447688\n",
      "Weight Norm: 69931.65625\n",
      "Bias Norm: 6556.53125\n",
      "iter98 Weight Norm / Bias Norm: 10.668158030833109\n",
      "Weight Norm: 70657.3515625\n",
      "Bias Norm: 6623.20068359375\n",
      "iter99 Weight Norm / Bias Norm: 10.67029816036661\n",
      "Weight Norm: 71383.0546875\n",
      "Bias Norm: 6689.8837890625\n",
      "[('0.weight', Parameter containing:\n",
      "tensor([[-170.8217, -163.9584,  227.6301,  ...,   58.8647,  -76.9027,\n",
      "         -204.0070],\n",
      "        [-142.1640,  -83.9131,  247.5322,  ...,  -12.9991, -109.5809,\n",
      "         -168.4617],\n",
      "        [  41.7213, -251.4654, -189.5764,  ...,   -9.1668,  202.0934,\n",
      "         -127.6209],\n",
      "        ...,\n",
      "        [  79.3519, -269.8991, -217.4850,  ...,  -54.5315,  207.0678,\n",
      "         -148.8349],\n",
      "        [  71.8671, -266.9869, -223.9558,  ..., -178.6960,  182.5872,\n",
      "         -166.0490],\n",
      "        [  77.7318, -266.1357, -224.7311,  ..., -167.1265,  203.7649,\n",
      "         -149.4624]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([303.8003, 304.0598, 306.4083, 305.9557, 306.2375, 306.7284, 304.2051,\n",
      "        306.4987, 306.0717, 306.6546, 306.3124, 306.3379, 306.2792, 304.0750,\n",
      "        304.1505, 305.2096, 306.2780, 305.0249, 306.5441, 304.3148, 303.8614,\n",
      "        306.1566, 306.4500, 303.8362, 305.9544, 306.0023, 306.3707, 306.2279,\n",
      "        304.3685, 304.1089, 306.1487, 303.8269, 303.9483, 306.3437, 306.0050,\n",
      "        306.2319, 304.2869, 306.1281, 304.2603, 306.4515, 306.5651, 305.9192,\n",
      "        306.0730, 304.4254, 304.6374, 304.5912, 306.6206, 304.8380, 306.1661,\n",
      "        306.6443, 305.3025, 304.7774, 304.0630, 304.1532, 304.9345, 306.3662,\n",
      "        303.7308, 306.4181, 306.3485, 306.3283, 305.2985, 304.2648, 305.1257,\n",
      "        304.2002, 304.0865, 306.6426, 305.9919, 306.5374, 306.2370, 304.0916,\n",
      "        304.0389, 303.8901, 303.6776, 304.4101, 306.3676, 306.2372, 304.5999,\n",
      "        306.4213, 306.5244, 306.4469, 304.4906, 304.2527, 303.9452, 304.2558,\n",
      "        306.4859, 306.1165, 306.3108, 306.5603, 306.7492, 304.2518, 304.8368,\n",
      "        304.0106, 303.8477, 304.0729, 306.3922, 306.4326, 304.2972, 306.6691,\n",
      "        306.2398, 303.9039, 306.5340, 304.0850, 306.2842, 306.1264, 306.4550,\n",
      "        306.2558, 306.4355, 306.2933, 303.7386, 304.7943, 306.0366, 304.4770,\n",
      "        304.0870, 303.8833, 304.1373, 306.4158, 306.5056, 304.1754, 304.7379,\n",
      "        304.1958, 304.6157, 303.8691, 303.9785, 304.0813, 306.1800, 306.4849,\n",
      "        306.5859, 305.3543, 304.5153, 306.4027, 304.6785, 303.7637, 303.6925,\n",
      "        303.7502, 305.7186, 304.9364, 306.1893, 303.9567, 306.4296, 304.3539,\n",
      "        305.9465, 305.1694, 303.9928, 303.8931, 306.6415, 303.8161, 305.8215,\n",
      "        306.3021, 304.1561, 305.5077, 306.3372, 304.2335, 303.9157, 304.5281,\n",
      "        306.6517, 305.1288, 306.3688, 305.0231, 305.6201, 306.2355, 305.8159,\n",
      "        304.3309, 306.5123, 306.5256, 306.3253, 305.0164, 304.7171, 305.9221,\n",
      "        303.8956, 305.3125, 305.0589, 306.2548, 306.4582, 304.0191, 306.0226,\n",
      "        303.9638, 306.2996, 304.1194, 306.4146, 306.5801, 305.5029, 306.5995,\n",
      "        306.2818, 306.2711, 306.3333, 306.4136, 305.7037, 304.2404, 306.5432,\n",
      "        306.3937, 304.1309, 303.9935, 306.6941, 306.6126, 306.2390, 303.7152,\n",
      "        304.0575, 304.0031, 303.6800, 305.7951, 306.3243, 304.1830, 304.0795,\n",
      "        306.0952, 305.5201, 306.3427, 306.6639, 306.3459, 306.7386, 305.9012,\n",
      "        304.2339, 306.3900, 305.6263, 305.6278, 306.3053, 304.0358, 306.5081,\n",
      "        305.1318, 304.0372, 306.5404, 306.1031, 304.8606, 306.6372, 304.0354,\n",
      "        306.4211, 303.9038, 305.8169, 306.3338, 306.4462, 305.1570, 304.7374,\n",
      "        306.1819, 306.3704, 304.5642, 304.2297, 306.4160, 305.9730, 305.1166,\n",
      "        305.9543, 306.4096, 304.3441, 305.5655, 305.9509, 306.0069, 306.4639,\n",
      "        304.0718, 306.5594, 306.2012, 304.0555, 306.3834, 306.4427, 304.8606,\n",
      "        303.7577, 306.2881, 306.3179, 306.3689], requires_grad=True)), ('3.weight', Parameter containing:\n",
      "tensor([[297.2366, 296.9535, 289.8166,  ..., 291.8113, 291.4420, 291.7822],\n",
      "        [ -8.3322, -21.5435, -39.8219,  ..., -26.4272, -22.2693, -13.8312],\n",
      "        [297.2347, 296.8753, 289.8134,  ..., 291.7940, 291.4296, 291.7732],\n",
      "        ...,\n",
      "        [295.8019, 295.4736, 288.3593,  ..., 290.3255, 290.0941, 290.3358],\n",
      "        [  4.9128,  -6.7698,  -3.8687,  ...,  -1.6515,  -7.9901,  -9.2295],\n",
      "        [295.8468, 295.6200, 288.4654,  ..., 290.4374, 290.1557, 290.4081]],\n",
      "       requires_grad=True)), ('3.bias', Parameter containing:\n",
      "tensor([305.4667, -10.9006, 305.4887, 303.0494, 288.0032, -19.9636, 305.5446,\n",
      "        305.5338, 305.4829, 304.3566, 305.5164, 305.5395, 296.2302, 305.5854,\n",
      "        305.5881, 305.4478, -20.6457, 305.4588, 305.5010, 305.5360, 304.0339,\n",
      "        303.8618, 305.5753, -13.3751, 303.9864, 300.0614, 304.2363, 305.4689,\n",
      "        305.5417, 305.5025, 303.1671, 304.2739, 305.5090, 305.5311, 305.5284,\n",
      "        302.9448, 305.5237, 305.5010, 305.5727, 305.5234, 305.5333, 305.4687,\n",
      "        305.4726, 305.4539, 303.0984, 305.5024, 305.5443, 305.5338, 305.4704,\n",
      "        305.5546, 305.5471, 305.4817, 305.5209, 305.4677, 305.5545, 302.5909,\n",
      "        305.4637, 305.5583, -22.3596, -15.4004, 305.5455, 305.5438, 302.6382,\n",
      "        302.7766, 305.4552, 305.4879, 305.4823, 305.4961, 305.5231, 305.4886,\n",
      "        305.5625, 302.8542, -20.8908, 303.9572, 305.5946, 305.4985, 305.5852,\n",
      "        -22.2958, 305.4829, 305.0695, -20.9301, -15.2888, -15.2583, 305.4969,\n",
      "        305.4555, 305.5875, -22.2767, 305.5103, 305.5377, 305.5182, 305.5446,\n",
      "        305.5451, 305.5500, 305.5053, 305.5370, 305.5279, 305.5491, 303.8653,\n",
      "        305.5145, 304.3149, -15.3950, 303.6989, 305.4494, 305.5795, 304.1209,\n",
      "        304.2463, 305.5538, 305.5270, 305.5110, -14.0122, 305.5211, 304.8724,\n",
      "        305.5763, 305.5280, -14.0331, 305.5056, 304.0033, 304.3558, 305.4657,\n",
      "        305.4708, 301.0258, 304.1736, -20.5969, 302.7431, 305.4623, -19.9028,\n",
      "        302.9536, 305.5258, -21.5777, -22.3265, 303.8090, 305.5095, 305.5015,\n",
      "        305.4828, 305.5435, 305.5941, 305.5446, 305.3972, 305.5018, 305.5286,\n",
      "        305.5693, 303.0693, 304.4111, 305.5491, -20.5365, 305.5071, 304.2606,\n",
      "        305.5724, 299.6068, 305.5082, 304.6007, 305.5197, 305.5880, -15.4150,\n",
      "        304.6245, 303.8028, 304.3326, 305.4538, 263.8930, -15.4617, 305.5790,\n",
      "        303.9886, 303.3112, 305.4949, 303.0471, 305.5123, 305.4928, 305.5575,\n",
      "        305.4967, 305.5874, 305.4804, 301.7538, -21.5853, 305.5638, 305.4716,\n",
      "        305.4556, 305.5504, 304.1371, 305.5689, 304.0655, 305.4597, 304.5033,\n",
      "        304.0349, 304.8557, -15.3856, 304.2511, 303.9457, 305.4767, 305.5221,\n",
      "        305.5070, 304.6229, 303.1415, 305.5331, 304.2937, 305.5612, 305.5914,\n",
      "        295.2066, -21.9390, 305.5933, 305.5911, 305.4826, 294.5461, 305.5187,\n",
      "        303.9730, 305.5493, 305.5374, 305.5143, 303.2282, 305.5533, 305.5466,\n",
      "        304.6286, 305.5207, -22.2101, 305.4770, 305.5883, 305.4327, -24.2378,\n",
      "        305.4811, 305.5474, 303.8650, 305.4756, 305.5174, 302.7531, -21.5951,\n",
      "        305.5254, 305.5701, 304.3529, 305.4882, 304.1356, 301.2939, 305.5027,\n",
      "        304.3314, 297.4012, 305.5749, -14.6539, 305.4706, 305.5773, 305.5652,\n",
      "        305.5548, 305.5221, 301.7172, 305.5030, -15.4423, 305.5643, 305.5650,\n",
      "        305.5133, 303.1797, 305.5057, 305.5161, 305.5855, -20.5515, 305.5586,\n",
      "        304.9565, 304.2003, -22.1890, 304.3625], requires_grad=True)), ('6.weight', Parameter containing:\n",
      "tensor([[249.4021, -46.8056, 249.3365, 248.3614, 235.6725, -37.1324, 249.3848,\n",
      "         249.3315, 249.3715, 248.0697, 249.4068, 249.3893, 243.5935, 249.3642,\n",
      "         249.3579, 249.4216, -39.9186, 249.4073, 249.4132, 249.3638, 247.7298,\n",
      "         247.5876, 249.3711, -51.0664, 247.6632, 247.2863, 247.9152, 249.3850,\n",
      "         249.3798, 249.3728, 246.8527, 247.9969, 249.4097, 249.3701, 249.3575,\n",
      "         248.2326, 249.3824, 249.3533, 249.4020, 249.3463, 249.3941, 249.4255,\n",
      "         249.4200, 249.4042, 246.8052, 249.3946, 249.3950, 249.3736, 249.3862,\n",
      "         249.3884, 249.3607, 249.3753, 249.4195, 249.4151, 249.3584, 247.9069,\n",
      "         249.3863, 249.3759, -36.5260, -53.0238, 249.3599, 249.4218, 247.9309,\n",
      "         248.0773, 249.4112, 249.4127, 249.4021, 249.4168, 249.3760, 249.4157,\n",
      "         249.3918, 248.1736, -40.2373, 247.6422, 249.3660, 249.3653, 249.3581,\n",
      "         -36.5050, 249.3666, 248.8167, -39.1079, -53.0490, -52.9919, 249.4169,\n",
      "         249.4080, 249.3719, -36.4832, 249.3126, 249.3688, 249.4232, 249.4099,\n",
      "         249.3916, 249.3342, 249.3793, 249.3742, 249.3689, 249.4256, 247.6083,\n",
      "         249.4002, 248.0932, -53.0628, 247.4378, 249.3330, 249.3551, 247.8487,\n",
      "         247.9938, 249.3780, 249.3896, 249.3528, -51.9644, 249.3920, 248.5674,\n",
      "         249.3628, 249.3600, -52.0196, 249.4161, 247.7211, 248.1376, 249.4070,\n",
      "         249.3797, 247.9655, 247.9069, -39.5316, 248.1049, 249.2859, -36.7827,\n",
      "         248.3200, 249.3779, -37.0217, -36.4555, 247.5397, 249.3752, 249.3938,\n",
      "         249.3774, 249.3649, 249.3682, 249.3576, 249.2704, 249.3770, 249.4035,\n",
      "         249.3756, 246.7909, 248.1847, 249.3787, -39.7211, 249.3974, 247.9481,\n",
      "         249.3945, 246.8624, 249.3547, 248.3498, 249.4068, 249.3673, -52.9700,\n",
      "         248.4359, 247.4752, 248.0166, 249.2773, 219.6103, -52.8682, 249.3684,\n",
      "         247.6748, 247.0170, 249.4087, 246.8023, 249.3637, 249.3549, 249.4018,\n",
      "         249.3728, 249.3736, 249.4034, 247.5422, -37.1357, 249.3920, 249.3553,\n",
      "         249.4082, 249.3797, 247.8630, 249.3768, 247.8327, 249.4192, 248.1953,\n",
      "         247.7388, 248.6644, -52.8117, 247.9927, 247.6408, 249.3664, 249.3907,\n",
      "         249.3963, 248.4505, 246.8525, 249.4208, 248.0205, 249.3955, 249.3701,\n",
      "         242.5504, -36.6871, 249.3642, 249.3659, 249.4159, 241.9434, 249.3951,\n",
      "         247.6919, 249.3593, 249.3706, 249.4158, 246.9518, 249.3650, 249.3768,\n",
      "         248.4340, 249.4169, -36.5489, 249.3909, 249.3753, 249.2027, -40.6398,\n",
      "         249.3401, 249.3972, 247.5905, 249.4259, 249.4134, 248.0559, -33.6766,\n",
      "         249.3797, 249.3930, 248.0544, 249.3682, 247.7834, 248.1171, 249.3711,\n",
      "         248.0432, 244.7821, 249.4081, -52.5478, 249.3779, 249.3689, 249.4083,\n",
      "         249.3778, 249.4188, 247.4975, 249.3840, -52.9128, 249.4225, 249.3884,\n",
      "         249.3302, 246.9584, 249.3656, 249.3828, 249.3765, -39.4403, 249.3923,\n",
      "         248.7220, 247.9342, -36.6893, 248.0527]], requires_grad=True)), ('6.bias', Parameter containing:\n",
      "tensor([304.0704], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through named parameters to compute norms for weights and biases separately\n",
    "# weight_norm = 0\n",
    "# bias_norm = 0\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'weight' in name:\n",
    "#         weight_norm += torch.norm(param).item()\n",
    "#     elif 'bias' in name:\n",
    "#         bias_norm += torch.norm(param).item()\n",
    "\n",
    "# Concatenate all weight and bias parameters\n",
    "for i in range(len(model_list)):\n",
    "    critic_param = model_list[i]\n",
    "    model = Critic(state_dim, action_dim, layernorm=0).q1\n",
    "    restore_state_dict(model, critic_param)\n",
    "\n",
    "    all_weights = torch.cat([param.view(-1) for name, param in model.named_parameters() if 'weight' in name])\n",
    "    all_biases = torch.cat([param.view(-1) for name, param in model.named_parameters() if 'bias' in name])\n",
    "\n",
    "    # Compute their norms\n",
    "    weight_norm = torch.norm(all_weights).item()\n",
    "    bias_norm = torch.norm(all_biases).item()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'iter{i}', \"Weight Norm / Bias Norm:\", weight_norm / bias_norm)\n",
    "    print(\"Weight Norm:\", weight_norm)\n",
    "    print(\"Bias Norm:\", bias_norm)\n",
    "    \n",
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntk cosine similarity tensor(1.)\n",
      "ntk norm rate tensor(9999.8936)\n",
      "grad cosine similarity tensor(1.)\n",
      "grad norm rate tensor(100.0004)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_ntk(model, x, x_prime):\n",
    "    N = x.size(0)\n",
    "\n",
    "    # Function to flatten gradients of model parameters\n",
    "    def flatten_grads(grads):\n",
    "        return torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "    # Compute gradients for each input\n",
    "    grads = []\n",
    "    next_grads = []\n",
    "    for i in range(N):\n",
    "\n",
    "        # Feedforward the input through the model\n",
    "        output = model(x[i])\n",
    "\n",
    "        # Zero the model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute and store the gradients for each output dimension\n",
    "        output.backward()\n",
    "        grad = flatten_grads([p.grad for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "        # Stack gradients for the current input\n",
    "        grads.append(grad)\n",
    "        \n",
    "        # Feedforward the input through the model\n",
    "        output = model(x_prime[i])\n",
    "\n",
    "        # Zero the model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute and store the gradients for each output dimension\n",
    "        output.backward()\n",
    "        grad = flatten_grads([p.grad for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "        # Stack gradients for the current input\n",
    "        next_grads.append(grad)\n",
    "        \n",
    "\n",
    "    # Stack gradients for all inputs\n",
    "    # divide a constant to prevent NAN\n",
    "    grads_tensor = torch.stack(grads) / (grad.shape[0])**0.5\n",
    "    next_grads_tensor = torch.stack(next_grads) / (grad.shape[0])**0.5\n",
    "\n",
    "    # Compute the NTK matrix using tensor operations\n",
    "    G = torch.matmul(grads_tensor, next_grads_tensor.t())\n",
    "\n",
    "    return G.detach(), grads_tensor.detach(), next_grads_tensor.detach()\n",
    "\n",
    "\n",
    "\n",
    "restore_state_dict(model, model_list[-1])\n",
    "ntk, grad, next_grad = compute_ntk(model, X, X_prime)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.data *= lambda_scale\n",
    "    \n",
    "    \n",
    "scaled_ntk, scaled_grad, scaled_next_grad = compute_ntk(model, X, X_prime)\n",
    "\n",
    "# reshape ntk; print similarity between ntk and scaled ntk\n",
    "ntk = ntk.reshape(-1)\n",
    "next_ntk = scaled_ntk.reshape(-1)\n",
    "print('ntk cosine similarity', torch.cosine_similarity(ntk, next_ntk, dim=0))\n",
    "print('ntk norm rate', torch.norm(next_ntk) / torch.norm(ntk))\n",
    "\n",
    "# reshape grad; print similarity between grad and scaled grad\n",
    "grad = grad.reshape(-1)\n",
    "next_grad = scaled_grad.reshape(-1)\n",
    "print('grad cosine similarity', torch.cosine_similarity(grad, next_grad, dim=0))\n",
    "print('grad norm rate', torch.norm(next_grad) / torch.norm(grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100]) torch.Size([100, 100])\n",
      "tensor([[ 6.8981e+08,  1.9389e+08,  1.9291e+08,  2.5750e+07],\n",
      "        [ 1.3779e+09,  7.4497e+08,  2.1770e+08,  4.8533e+08],\n",
      "        [ 3.8562e+08, -1.8764e+08,  6.0813e+08,  4.1301e+08],\n",
      "        [ 2.1829e+09,  1.4552e+09,  1.4245e+09,  7.3954e+08]])\n",
      "tensor([[1.4078e+08, 1.4504e+08, 8.5961e+07, 1.3817e+08],\n",
      "        [2.1403e+08, 2.1936e+08, 2.0636e+08, 6.7178e+07],\n",
      "        [2.0130e+08, 2.0130e+08, 1.5371e+08, 1.2437e+08],\n",
      "        [2.8615e+08, 2.9148e+08, 2.3240e+08, 1.3924e+08]])\n",
      "weight/bias NTK rate tensor(6.6623)\n",
      "tensor([[3.1570e+12, 1.1195e+12, 2.0930e+12, 9.4983e+11, 1.9378e+12, 4.8247e+11,\n",
      "         1.4625e+12, 6.0053e+11, 1.2362e+12, 4.9715e+11],\n",
      "        [7.1594e+12, 3.8721e+12, 5.4845e+12, 4.7904e+12, 1.9393e+12, 3.4202e+12,\n",
      "         2.0803e+12, 1.5541e+12, 4.5866e+12, 5.9006e+12]]) tensor([[ 8.3059e+08,  3.3893e+08,  2.7887e+08,  1.6392e+08,  5.1979e+08,\n",
      "          2.0015e+08,  3.1466e+08,  4.9955e+07,  2.3031e+08,  5.7939e+07],\n",
      "        [ 1.5920e+09,  9.6433e+08,  4.2406e+08,  5.5250e+08,  2.8914e+08,\n",
      "          1.2410e+09,  2.4968e+08, -4.7400e+06,  7.0192e+08,  1.1892e+09]])\n"
     ]
    }
   ],
   "source": [
    "def compute_ntk(model, x, x_prime):\n",
    "    N = x.size(0)\n",
    "\n",
    "    # Function to flatten gradients of model parameters\n",
    "    def flatten_grads(grads):\n",
    "        return torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "    # Compute gradients for each input\n",
    "    weight_grads = []\n",
    "    bias_grads = []\n",
    "    next_weight_grads = []\n",
    "    next_bias_grads = []\n",
    "    for i in range(N):\n",
    "\n",
    "        # Feedforward the input through the model\n",
    "        output = model(x[i])\n",
    "\n",
    "        # Zero the model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute and store the gradients for each output dimension\n",
    "        output.backward()\n",
    "        weight_grad = flatten_grads([p.grad for name, p in model.named_parameters() if 'weight' in name and p.grad is not None])\n",
    "        bias_grad = flatten_grads([p.grad for name, p in model.named_parameters() if 'bias' in name and p.grad is not None])\n",
    "\n",
    "        # Stack gradients for the current input\n",
    "        weight_grads.append(weight_grad)\n",
    "        bias_grads.append(bias_grad)\n",
    "        \n",
    "        # Feedforward the input through the model\n",
    "        output = model(x_prime[i])\n",
    "\n",
    "        # Zero the model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute and store the gradients for each output dimension\n",
    "        output.backward()\n",
    "        next_weight_grad = flatten_grads([p.grad for name, p in model.named_parameters() if 'weight' in name and p.grad is not None])\n",
    "        next_bias_grad = flatten_grads([p.grad for name, p in model.named_parameters() if 'bias' in name and p.grad is not None])\n",
    "\n",
    "\n",
    "        # Stack gradients for the current input\n",
    "        next_weight_grads.append(next_weight_grad)\n",
    "        next_bias_grads.append(next_bias_grad)\n",
    "        \n",
    "\n",
    "    # Stack gradients for all inputs\n",
    "    # divide a constant to prevent NAN\n",
    "    size = weight_grad.shape[0] + bias_grad.shape[0]\n",
    "    weight_grads_tensor = torch.stack(weight_grads) / size**0.5\n",
    "    bias_grads_tensor = torch.stack(bias_grads) / size**0.5\n",
    "    next_weight_grads_tensor = torch.stack(next_weight_grads) / size**0.5\n",
    "    next_bias_grads_tensor = torch.stack(next_bias_grads) / size**0.5\n",
    "    # weight_grads_tensor = torch.stack(weight_grads) \n",
    "    # bias_grads_tensor = torch.stack(bias_grads) \n",
    "    # next_weight_grads_tensor = torch.stack(next_weight_grads)\n",
    "    # next_bias_grads_tensor = torch.stack(next_bias_grads)\n",
    "\n",
    "    # Compute the NTK matrix using tensor operations\n",
    "    G_weight = torch.matmul(weight_grads_tensor, next_weight_grads_tensor.t())\n",
    "    G_bias = torch.matmul(bias_grads_tensor, next_bias_grads_tensor.t())\n",
    "    print(G_weight.shape, G_bias.shape)\n",
    "    print(G_weight[:4, :4])\n",
    "    print(G_bias[:4, :4])\n",
    "    \n",
    "    # norm rate\n",
    "    print('weight/bias NTK rate', torch.norm(torch.diag(G_weight)) / torch.norm(torch.diag(G_bias)))\n",
    "\n",
    "    return G_weight.detach(), G_bias.detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "restore_state_dict(model, model_list[10])\n",
    "G_weight, G_bias = compute_ntk(model, X, X_prime)\n",
    "ntk2 = G_weight + G_bias\n",
    "\n",
    "# whether ntk equals to ntk2\n",
    "print(ntk.reshape((N,N))[:2, :10], ntk2[:2, :10])\n",
    "\n",
    "# compute_ntk(model, X, X)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.data *= lambda_scale\n",
    "    \n",
    "    \n",
    "# scaled_ntk, scaled_grad, scaled_next_grad = compute_ntk(model, X, X_prime)\n",
    "\n",
    "# # reshape ntk; print similarity between ntk and scaled ntk\n",
    "# ntk = ntk.reshape(-1)\n",
    "# next_ntk = scaled_ntk.reshape(-1)\n",
    "# print('ntk cosine similarity', torch.cosine_similarity(ntk, next_ntk, dim=0))\n",
    "# print('ntk norm rate', torch.norm(next_ntk) / torch.norm(ntk))\n",
    "\n",
    "# # reshape grad; print similarity between grad and scaled grad\n",
    "# grad = grad.reshape(-1)\n",
    "# next_grad = scaled_grad.reshape(-1)\n",
    "# print('grad cosine similarity', torch.cosine_similarity(grad, next_grad, dim=0))\n",
    "# print('grad norm rate', torch.norm(next_grad) / torch.norm(grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halfcheetah-medium-v2\n",
      "Cosine similarity: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Distance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "halfcheetah-medium-expert-v2\n",
      "Cosine similarity: [0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.99, 1.0, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 1.0, 0.99, 0.99, 1.0, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 0.99, 0.99, 0.99, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.99, 0.98, 0.97, 0.99, 0.99, 0.99]\n",
      "Distance: [0.08, 0.04, 0.05, 0.04, 0.03, 0.06, 0.05, 0.03, 0.04, 0.05, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.02, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.02, 0.02, 0.03, 0.04, 0.03, 0.04, 0.02, 0.02, 0.04, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.04, 0.04, 0.03, 0.04, 0.04, 0.06, 0.04, 0.04, 0.03, 0.04, 0.04, 0.04, 0.03, 0.04, 0.04, 0.03, 0.04, 0.04, 0.04, 0.05, 0.05, 0.04, 0.04, 0.05, 0.04, 0.06, 0.03, 0.04, 0.05, 0.03, 0.04, 0.03, 0.04, 0.06, 0.02, 0.03, 0.04, 0.02, 0.04, 0.04, 0.04, 0.02, 0.02, 0.02, 0.04, 0.04, 0.04, 0.02, 0.03, 0.03, 0.02, 0.02, 0.03, 0.04, 0.1, 0.15, 0.04, 0.04, 0.04]\n",
      "halfcheetah-medium-replay-v2\n",
      "Cosine similarity: [1.0, 1.0, 1.0, 0.73, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.89, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99, 0.99, 0.98, 0.98, 0.98, 0.99, 0.98, 0.98, 0.98, 0.99, 0.98, 0.98, 0.99, 0.99, 0.98, 0.98, 0.99, 0.98, 0.99, 0.98, 0.98, 0.99, 0.98, 0.98, 0.98, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.98, 0.97, 0.98, 0.98, 0.99, 0.98, 0.98, 0.97, 0.97, 0.99, 0.98, 0.98, 0.97, 0.98, 0.99, 0.99, 0.99, 0.98, 0.98, 0.97, 0.98]\n",
      "Distance: [0.0, 0.0, 0.0, 1.23, 0.07, 0.09, 0.05, 0.05, 0.04, 0.04, 0.06, 0.05, 0.07, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.07, 0.05, 0.07, 0.06, 0.09, 0.65, 0.13, 0.12, 0.11, 0.09, 0.06, 0.07, 0.1, 0.1, 0.09, 0.07, 0.06, 0.1, 0.11, 0.09, 0.06, 0.07, 0.11, 0.11, 0.09, 0.08, 0.1, 0.1, 0.11, 0.08, 0.08, 0.1, 0.07, 0.08, 0.09, 0.1, 0.07, 0.08, 0.08, 0.1, 0.11, 0.07, 0.09, 0.11, 0.12, 0.14, 0.1, 0.09, 0.1, 0.1, 0.11, 0.1, 0.09, 0.08, 0.11, 0.07, 0.08, 0.08, 0.06, 0.1, 0.11, 0.09, 0.09, 0.06, 0.08, 0.08, 0.13, 0.13, 0.08, 0.08, 0.1, 0.12, 0.08, 0.07, 0.06, 0.06, 0.07, 0.1, 0.12, 0.08]\n",
      "antmaze-large-diverse-v0\n",
      "Cosine similarity: [0.31, 0.47, 0.56, 0.44, 0.49, 0.39, 0.31, 0.38, 0.33, 0.5, 0.29, 0.4, 0.36, 0.34, 0.34, 0.39, 0.26, 0.26, 0.39, 0.42, 0.33, 0.41, 0.36, 0.47, 0.38, 0.46, 0.32, 0.23, 0.96, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 0.97, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99]\n",
      "Distance: [2.95, 2.53, 2.26, 2.66, 2.55, 2.83, 3.03, 2.87, 2.98, 2.54, 3.15, 2.82, 2.94, 2.95, 2.94, 2.89, 3.21, 3.21, 2.91, 2.81, 3.03, 2.82, 2.97, 2.65, 2.91, 2.65, 3.11, 3.37, 0.34, 0.13, 0.09, 0.07, 0.05, 0.07, 0.05, 0.05, 0.07, 0.04, 0.05, 0.04, 0.06, 0.05, 0.05, 0.22, 0.08, 0.07, 0.05, 0.06, 0.05, 0.05, 0.05, 0.07, 0.05, 0.06, 0.06, 0.05, 0.04, 0.04, 0.07, 0.06, 0.04, 0.04, 0.04, 0.05, 0.06, 0.06, 0.04, 0.05, 0.04, 0.05, 0.05, 0.05, 0.06, 0.04, 0.04, 0.07, 0.05, 0.04, 0.04, 0.05, 0.04, 0.04, 0.06, 0.04, 0.03, 0.04, 0.04, 0.04, 0.06, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04, 0.05, 0.04, 0.04, 0.05]\n",
      "antmaze-medium-play-v0\n",
      "Cosine similarity: [-0.58, 0.54, 0.99, 0.49, 0.45, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Distance: [4.95, 2.66, 0.07, 2.84, 2.95, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "antmaze-medium-diverse-v0\n",
      "Cosine similarity: [0.18, 0.33, -0.09, 0.17, -0.03, 0.36, 0.36, 0.27, 0.29, 0.04, 0.78, 0.62, 0.51, 0.32, 0.25, 0.42, 0.47, 0.61, 0.47, 0.36, 0.17, 0.65, 0.37, 0.23, 0.36, 0.41, 0.47, 0.83, 0.49, 0.56, 0.57, 0.73, 0.8, 0.68, 0.8, 0.71, 0.72, 0.73, 0.75, 0.72, 0.86, 0.73, 0.69, 0.54, 0.68, 0.63, 0.49, 0.69, 0.63, 0.59, 0.66, 0.72, 0.74, 0.55, 0.64, 0.63, 0.62, 0.64, 0.58, 0.57, 0.62, 0.63, 0.61, 0.72, 0.7, 0.64, 0.64, 0.6, 0.62, 0.66, 0.58, 0.67, 0.61, 0.55, 0.59, 0.38, 0.32, 0.5, 0.62, 0.7, 0.56, 0.5, 0.27, 0.01, 0.52, 0.35, 0.6, 0.35, 0.37, 0.51, 0.52, 0.57, 0.5, 0.19, 0.32, 0.28, 0.15, 0.27, 0.33]\n",
      "Distance: [3.57, 3.2, 4.13, 3.62, 4.03, 3.05, 3.04, 3.31, 3.3, 3.87, 1.37, 2.19, 2.57, 3.19, 3.38, 2.91, 2.8, 2.28, 2.72, 3.11, 3.59, 1.9, 3.08, 3.39, 3.1, 2.89, 2.71, 1.27, 2.8, 2.49, 2.45, 1.71, 1.32, 1.92, 1.34, 1.87, 1.75, 1.63, 1.58, 1.76, 1.02, 1.8, 1.99, 2.54, 1.91, 2.23, 2.66, 1.91, 2.21, 2.24, 1.98, 1.72, 1.66, 2.43, 2.09, 2.12, 2.15, 2.08, 2.29, 2.3, 2.1, 2.14, 2.24, 1.71, 1.83, 2.02, 2.02, 2.17, 2.09, 1.99, 2.24, 1.97, 2.19, 2.44, 2.3, 3.01, 3.2, 2.64, 2.13, 1.83, 2.33, 2.54, 3.35, 3.9, 2.54, 3.09, 2.26, 3.05, 2.95, 2.59, 2.54, 2.36, 2.57, 3.5, 3.15, 3.2, 3.59, 3.26, 3.13]\n",
      "antmaze-large-play-v0\n",
      "Cosine similarity: [0.23, 0.18, 0.41, 0.39, 0.39, 0.45, 0.5, 0.31, 0.24, 0.43, 0.48, 0.38, 0.34, 0.39, 0.43, 0.36, 0.45, 0.4, 0.42, 0.42, 0.42, 0.4, 0.47, 0.48, 0.39, 0.43, 0.42, 0.46, 0.44, 0.44, 0.38, 0.36, 0.33, 0.36, 0.37, 0.41, 0.45, 0.35, 0.39, 0.38, 0.45, 0.44, 0.36, 0.49, 0.29, 0.39, 0.33, 0.34, 0.42, 0.43, 0.39, 0.29, 0.4, 0.41, 0.47, 0.44, 0.45, 0.51, 0.4, 0.39, 0.38, 0.38, 0.56, 0.39, 0.38, 0.39, 0.41, 0.43, 0.52, 0.46, 0.49, 0.47, 0.34, 0.34, 0.49, 0.44, 0.31, 0.36, 0.23, 0.34, 0.24, 0.37, 0.37, 0.36, 0.42, 0.43, 0.44, 0.39, 0.36, 0.39, 0.52, 0.38, 0.48, 0.41, 0.47, 0.46, 0.48, 0.52, 0.53]\n",
      "Distance: [3.15, 3.28, 2.73, 2.82, 2.84, 2.7, 2.52, 3.03, 3.2, 2.75, 2.6, 2.85, 3.03, 2.87, 2.75, 2.95, 2.7, 2.82, 2.76, 2.75, 2.76, 2.81, 2.66, 2.63, 2.84, 2.75, 2.75, 2.65, 2.7, 2.75, 2.9, 2.95, 3.04, 2.93, 2.92, 2.79, 2.73, 2.96, 2.84, 2.9, 2.69, 2.72, 2.92, 2.54, 3.1, 2.87, 3.0, 3.0, 2.81, 2.77, 2.84, 3.15, 2.84, 2.8, 2.59, 2.72, 2.66, 2.52, 2.8, 2.84, 2.86, 2.83, 2.34, 2.8, 2.87, 2.8, 2.79, 2.73, 2.45, 2.65, 2.56, 2.6, 2.93, 3.0, 2.59, 2.69, 3.09, 2.96, 3.29, 3.03, 3.25, 2.93, 2.91, 2.93, 2.8, 2.73, 2.71, 2.83, 2.97, 2.88, 2.47, 2.92, 2.58, 2.74, 2.62, 2.68, 2.61, 2.48, 2.47]\n",
      "antmaze-large-diverse-v0\n",
      "Cosine similarity: [0.31, 0.47, 0.56, 0.44, 0.49, 0.39, 0.31, 0.38, 0.33, 0.5, 0.29, 0.4, 0.36, 0.34, 0.34, 0.39, 0.26, 0.26, 0.39, 0.42, 0.33, 0.41, 0.36, 0.47, 0.38, 0.46, 0.32, 0.23, 0.96, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 0.97, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 0.99, 0.99, 1.0, 0.99, 1.0, 0.99, 0.99, 0.99, 0.99]\n",
      "Distance: [2.95, 2.53, 2.26, 2.66, 2.55, 2.83, 3.03, 2.87, 2.98, 2.54, 3.15, 2.82, 2.94, 2.95, 2.94, 2.89, 3.21, 3.21, 2.91, 2.81, 3.03, 2.82, 2.97, 2.65, 2.91, 2.65, 3.11, 3.37, 0.34, 0.13, 0.09, 0.07, 0.05, 0.07, 0.05, 0.05, 0.07, 0.04, 0.05, 0.04, 0.06, 0.05, 0.05, 0.22, 0.08, 0.07, 0.05, 0.06, 0.05, 0.05, 0.05, 0.07, 0.05, 0.06, 0.06, 0.05, 0.04, 0.04, 0.07, 0.06, 0.04, 0.04, 0.04, 0.05, 0.06, 0.06, 0.04, 0.05, 0.04, 0.05, 0.05, 0.05, 0.06, 0.04, 0.04, 0.07, 0.05, 0.04, 0.04, 0.05, 0.04, 0.04, 0.06, 0.04, 0.03, 0.04, 0.04, 0.04, 0.06, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04, 0.05, 0.04, 0.04, 0.05]\n"
     ]
    }
   ],
   "source": [
    "# Action similarity check\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from TD3_BC import Critic\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "\n",
    "for env_name in [\n",
    "                # 'hopper-medium-v2', 'hopper-medium-expert-v2',  'hopper-medium-replay-v2',\n",
    "                #  'walker2d-medium-v2', 'walker2d-medium-expert-v2', 'walker2d-medium-replay-v2',\n",
    "                'halfcheetah-medium-v2', 'halfcheetah-medium-expert-v2', 'halfcheetah-medium-replay-v2',\n",
    "    'antmaze-large-diverse-v0', 'antmaze-medium-play-v0', 'antmaze-medium-diverse-v0', 'antmaze-large-play-v0', 'antmaze-large-diverse-v0']:\n",
    "\n",
    "    # env = gym.make(env_name)\n",
    "    # state_dim = env.observation_space.shape[0]\n",
    "    # action_dim = env.action_space.shape[0] \n",
    "    # _max_action = float(env.action_space.high[0])\n",
    "\n",
    "    model_path = f'results/{env_name}.pt'\n",
    "    model_list = torch.load(model_path)['q1_models']\n",
    "    pi_list = torch.load(model_path)['pi_list']\n",
    "    \n",
    "    print(env_name)\n",
    "    sims = []\n",
    "    distances = []\n",
    "    for i in range(len(model_list)-1):\n",
    "        sims.append(round(torch.cosine_similarity(pi_list[i], pi_list[i+1], dim=1).mean().item(), 2))\n",
    "        # get the distance of corresponding vectors in the axis=1 and average them\n",
    "        distances.append(round(torch.norm(pi_list[i]-pi_list[i+1], dim=1).mean().round(decimals=2).item(), 2))\n",
    "    print(f'Cosine similarity: {sims}')\n",
    "    print(f'Distance: {distances}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
